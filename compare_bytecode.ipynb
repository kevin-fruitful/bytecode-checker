{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "import cbor2\n",
    "import base58\n",
    "import datetime\n",
    "import json\n",
    "import glob\n",
    "import requests\n",
    "from web3 import Web3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "from git import Repo\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess\n",
    "from IPython.display import display, Markdown\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# load .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondLoupeABI = [\n",
    "    {\n",
    "        \"constant\": True,\n",
    "        \"inputs\": [],\n",
    "        \"name\": \"facetAddresses\",\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"internalType\": \"address[]\",\n",
    "                \"name\": \"facetAddresses_\",\n",
    "                \"type\": \"address[]\",\n",
    "            }\n",
    "        ],\n",
    "        \"payable\": False,\n",
    "        \"stateMutability\": \"view\",\n",
    "        \"type\": \"function\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareBytecode:\n",
    "    def __init__(\n",
    "        self,\n",
    "        w3: Web3,\n",
    "        contract_abi: dict,\n",
    "        build_previous_commits: list = None,\n",
    "        dir_path_to_broadcasts: str = \"broadcast\",\n",
    "        dir_path_to_artifacts: str = \"forge-artifacts\",\n",
    "        additional_contracts: dict = None,\n",
    "        forge_path: str = None,\n",
    "        repo_path: str = None,\n",
    "    ):\n",
    "        self.is_run_locally = False\n",
    "        self.current_commit_sha = None\n",
    "        self.current_repo_path = None\n",
    "        self.set_commit_sha_and_execution_context(repo_path)\n",
    "\n",
    "        self.build_previous_commits = build_previous_commits\n",
    "\n",
    "        self.w3 = w3\n",
    "        self.contract_abi = contract_abi\n",
    "        self.dir_path_to_broadcasts = dir_path_to_broadcasts\n",
    "        self.dir_path_to_artifacts = dir_path_to_artifacts\n",
    "\n",
    "        self.diamond_address = self.get_address_proxy_diamond(\"contract_addresses.csv\")\n",
    "\n",
    "        self.forge_path = (\n",
    "            forge_path if forge_path is not None else shutil.which(\"forge\")\n",
    "        )\n",
    "        if self.forge_path is None:\n",
    "            raise Exception(\n",
    "                \"Could not find 'forge' executable. Please ensure it is installed and available in your PATH.\"\n",
    "            )\n",
    "\n",
    "        self.current_facet_addresses: list = None  # keeps track of facet addresses currently active on the proxy contract\n",
    "        self.contracts = (\n",
    "            self.match_contract_addresses_to_names()\n",
    "        )  # if there's no name to match, then this currently removes the contract from the list\n",
    "\n",
    "        # Add any additional self.contracts - override existing ones if needed\n",
    "        if additional_contracts:\n",
    "            for contract_name, address in additional_contracts.items():\n",
    "                if contract_name not in self.contracts:\n",
    "                    self.contracts[contract_name] = address\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Contract '{contract_name}' already exists in the dictionary.\"\n",
    "                    )\n",
    "\n",
    "        self.metadata: dict = {}\n",
    "\n",
    "    def set_commit_sha_and_execution_context(self, repo_path=None):\n",
    "        \"\"\"\n",
    "        Set the current commit SHA and the execution context (local or CI).\n",
    "\n",
    "        Args:\n",
    "            repo_path (str): local path to the Git repository. If None, it's assumed the script is running in CI.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if repo_path is given\n",
    "        if repo_path:\n",
    "            # Check if the repo_path exists and is a directory\n",
    "            if os.path.isdir(repo_path):\n",
    "                self.current_repo_path = repo_path\n",
    "                # We're probably running locally, so let's use a git command to get the commit SHA\n",
    "                try:\n",
    "                    self.current_commit_sha = (\n",
    "                        subprocess.check_output(\n",
    "                            [\"git\", \"-C\", repo_path, \"rev-parse\", \"HEAD\"]\n",
    "                        )\n",
    "                        .strip()\n",
    "                        .decode(\"utf-8\")\n",
    "                    )\n",
    "                    self.is_run_locally = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while trying to get commit SHA: {e}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Error: The provided path '{repo_path}' is not a valid directory.\"\n",
    "                )\n",
    "        else:\n",
    "            # We're probably running on GitHub Actions\n",
    "            self.current_repo_path = os.getenv(\n",
    "                \"GITHUB_WORKSPACE\"\n",
    "            )  # This is the default directory where GitHub Actions checks out your repository\n",
    "            self.current_commit_sha = os.getenv(\"GITHUB_SHA\")\n",
    "            if not self.current_repo_path or not self.current_commit_sha:\n",
    "                print(\n",
    "                    \"Warning: Script is not running in local environment or valid GitHub Actions environment.\"\n",
    "                )\n",
    "\n",
    "    def get_commit_timestamp(self, repo_path, commit_sha=None):\n",
    "        if commit_sha is None:\n",
    "            commit_sha = self.current_commit_sha\n",
    "\n",
    "        try:\n",
    "            commit_timestamp = (\n",
    "                subprocess.check_output(\n",
    "                    [\"git\", \"-C\", repo_path, \"show\", \"-s\", \"--format=%ci\", commit_sha]\n",
    "                )\n",
    "                .strip()\n",
    "                .decode(\"utf-8\")\n",
    "            )\n",
    "\n",
    "            # Parse the git timestamp format\n",
    "            dt = datetime.datetime.strptime(commit_timestamp, \"%Y-%m-%d %H:%M:%S %z\")\n",
    "\n",
    "            # Format as standard ISO 8601\n",
    "            iso8601_time = dt.isoformat()\n",
    "            return iso8601_time, commit_sha\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\n",
    "                f\"Error: Failed to get the commit timestamp. Check if the path '{repo_path}' contains a valid git repository and the commit SHA '{commit_sha}' is valid.\"\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "    def get_address_proxy_diamond(self, filename=None, chain_id=1):\n",
    "        \"\"\"\n",
    "        Reads a CSV file and returns the address of the proxy_diamond contract for the given chain id.\n",
    "\n",
    "        Returns:\n",
    "            str: The address of the proxy_diamond contract for the given chain id.\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(filename):\n",
    "            print(f\"File '{filename}' does not exist.\")\n",
    "            return None\n",
    "\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(filename)\n",
    "        # TODO change to all and handle multiple addresses\n",
    "        address = df.loc[\n",
    "            (df[\"chain_id\"] == chain_id) & (df[\"contract_type\"] == \"proxy_diamond\"),\n",
    "            \"proxy_address\",\n",
    "        ].item()\n",
    "\n",
    "        if address is None:\n",
    "            print(f\"No proxy_diamond found for chain id '{chain_id}' in {filename}.\")\n",
    "\n",
    "        return address\n",
    "\n",
    "    def perform_build(self, repo, commit):\n",
    "        \"\"\"\n",
    "        Performs the git clone, checkout, and forge build operations for the specified repo and commit.\n",
    "\n",
    "        Args:\n",
    "            repo (str): The repository to use.\n",
    "            commit (str): The commit to checkout.\n",
    "        \"\"\"\n",
    "        # Save the initial working directory\n",
    "        start_dir = os.getcwd()\n",
    "\n",
    "        # Create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            # Clone the repository into the temporary directory\n",
    "            clone_command = [\"git\", \"clone\", repo, tempdir]\n",
    "            subprocess.run(clone_command, check=True)\n",
    "\n",
    "            # Checkout the specific commit\n",
    "            os.chdir(tempdir)\n",
    "            checkout_command = [\"git\", \"checkout\", commit]\n",
    "            subprocess.run(checkout_command, check=True)\n",
    "\n",
    "            # Deinitialize any existing submodules\n",
    "            # deinit_command = [\"git\", \"submodule\", \"deinit\", \"--force\", \".\"]\n",
    "            # subprocess.run(deinit_command, check=True)\n",
    "\n",
    "            # Update and initialize submodules\n",
    "            update_command = [\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"]\n",
    "            subprocess.run(update_command, check=True)\n",
    "\n",
    "            # Create a new directory for these artifacts with the commit hash in the name\n",
    "            temp_artifacts_dir = os.path.join(tempdir, f\"{commit}_artifacts\")\n",
    "            os.makedirs(temp_artifacts_dir, exist_ok=True)\n",
    "\n",
    "            # Run `forge build`\n",
    "            build_command = [\n",
    "                self.forge_path,\n",
    "                \"build\",\n",
    "                \"--out\",\n",
    "                temp_artifacts_dir,\n",
    "                \"--skip\",\n",
    "                \"script\",\n",
    "                \"test\",\n",
    "                \"D0\",\n",
    "                \"DeploymentHelpers\",\n",
    "            ]\n",
    "            subprocess.run(build_command, check=True)\n",
    "\n",
    "            # Switch back to the initial directory\n",
    "            os.chdir(start_dir)\n",
    "\n",
    "            # Create a directory for these artifacts with the commit hash in the name in a persistent location\n",
    "            new_artifacts_dir = os.path.join(\n",
    "                start_dir, \"temp_artifacts\", f\"{commit}_artifacts\"\n",
    "            )\n",
    "            os.makedirs(new_artifacts_dir, exist_ok=True)\n",
    "\n",
    "            # Copy the files from the temporary directory to the new directory\n",
    "            shutil.copytree(temp_artifacts_dir, new_artifacts_dir, dirs_exist_ok=True)\n",
    "\n",
    "        return new_artifacts_dir\n",
    "\n",
    "    def get_current_facet_addresses(self, diamond_address=None):\n",
    "        if diamond_address is None:\n",
    "            diamond_address = self.diamond_address\n",
    "\n",
    "        diamond_loupe = self.w3.eth.contract(\n",
    "            address=diamond_address, abi=self.contract_abi\n",
    "        )\n",
    "\n",
    "        # Call the facetAddresses() method\n",
    "        try:\n",
    "            self.current_facet_addresses = (\n",
    "                diamond_loupe.functions.facetAddresses().call()\n",
    "            )\n",
    "\n",
    "            return self.current_facet_addresses\n",
    "\n",
    "        except:\n",
    "            print(\"Failed to call facetAddresses() method\")\n",
    "\n",
    "    def create_contract_address_list(self):\n",
    "        # Get the facet addresses\n",
    "        facet_addresses = self.get_current_facet_addresses()\n",
    "\n",
    "        contract_address_list = [self.diamond_address] + facet_addresses\n",
    "\n",
    "        return contract_address_list\n",
    "\n",
    "    def match_contract_addresses_to_names(self, contract_addresses: str = None):\n",
    "        \"\"\"\n",
    "        Create a dictionary of contract names and addresses.\n",
    "\n",
    "        Args:\n",
    "            contract_addresses (str): The addresses of the contract.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with contract names as keys and contract addresses as values.\n",
    "        \"\"\"\n",
    "        contract_dict = {}\n",
    "        reverse_dict = {}\n",
    "\n",
    "        # Get the contract addresses\n",
    "        if contract_addresses is None:\n",
    "            contract_addresses = self.create_contract_address_list()\n",
    "\n",
    "        # Traverse the directory and find all JSON files\n",
    "        root = self.dir_path_to_broadcasts\n",
    "        json_files = []\n",
    "        for dirpath, dirnames, filenames in os.walk(root):\n",
    "            # check if directory ends with key_to_read_diamond_address\n",
    "            # TODO chain_id\n",
    "            if dirpath.endswith(str(self.w3.eth.chain_id)):\n",
    "                # check if directory is not 'dry-run'\n",
    "                if \"dry-run\" not in dirpath:\n",
    "                    for filename in filenames:\n",
    "                        if filename.endswith(\".json\"):\n",
    "                            json_path = os.path.join(dirpath, filename)\n",
    "                            json_files.append(json_path)\n",
    "\n",
    "        # Sort the files by modification time in reverse order\n",
    "        sorted_files = sorted(json_files, key=os.path.getmtime, reverse=True)\n",
    "        for json_path in sorted_files:\n",
    "            with open(json_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                transactions = data.get(\"transactions\", [])\n",
    "                for transaction in transactions:\n",
    "                    # Check if the contractAddress matches any of the facet addresses\n",
    "                    if transaction.get(\"contractAddress\") in contract_addresses:\n",
    "                        contract_name = transaction.get(\"contractName\")\n",
    "                        contract_address = transaction.get(\"contractAddress\")\n",
    "                        # If address not in contract_dict or is same as existing key\n",
    "                        if (\n",
    "                            contract_address not in contract_dict.values()\n",
    "                            or contract_dict.get(contract_name) == contract_address\n",
    "                        ):\n",
    "                            contract_dict[contract_name] = contract_address\n",
    "                            # Update the reverse dictionary\n",
    "                            if (\n",
    "                                contract_address in reverse_dict\n",
    "                                and contract_name != reverse_dict[contract_address]\n",
    "                            ):\n",
    "                                reverse_dict[contract_address].add(contract_name)\n",
    "                            else:\n",
    "                                reverse_dict[contract_address] = {contract_name}\n",
    "                        else:\n",
    "                            duplicate_keys = [\n",
    "                                k\n",
    "                                for k, v in contract_dict.items()\n",
    "                                if v == contract_address\n",
    "                            ]\n",
    "                            print(\n",
    "                                f\"Duplicate address {contract_address} found for contracts: {duplicate_keys} and {contract_name} in {filename}\"\n",
    "                            )\n",
    "                            duplicate_keys.append(contract_name)\n",
    "                            print(\n",
    "                                \"Which contract would you like to provide a new address for?\"\n",
    "                            )\n",
    "                            for i, key in enumerate(duplicate_keys):\n",
    "                                print(f\"{i+1}: {key}\")\n",
    "                            selected_index = (\n",
    "                                int(\n",
    "                                    input(\n",
    "                                        \"Enter the number corresponding to your selection: \"\n",
    "                                    )\n",
    "                                )\n",
    "                                - 1\n",
    "                            )\n",
    "                            selected_key = duplicate_keys[selected_index]\n",
    "                            new_address = input(\n",
    "                                \"Enter new address for the selected contract: \"\n",
    "                            )\n",
    "                            contract_dict[selected_key] = new_address\n",
    "\n",
    "        return contract_dict\n",
    "\n",
    "    def verify_contracts_are_active(self, contracts: dict = None):\n",
    "        \"\"\"\n",
    "        Verifies that the contract is active by calling the `getFacetAddresses()` method.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the facet addresses\n",
    "        facet_addresses = self.get_current_facet_addresses()\n",
    "\n",
    "        if contracts is None:\n",
    "            contracts = self.contracts\n",
    "\n",
    "        active_contracts: dict = {}\n",
    "\n",
    "        # Check if the contract address is in the list of facet addresses\n",
    "        for contract_name, contract_address in contracts.items():\n",
    "            if contract_address in facet_addresses:\n",
    "                active_contracts[contract_name] = True\n",
    "            else:\n",
    "                active_contracts[contract_name] = False\n",
    "\n",
    "            # Assume the proxy address in self.contracts is active\n",
    "            if contract_address == self.diamond_address:\n",
    "                active_contracts[contract_name] = True\n",
    "\n",
    "        return active_contracts\n",
    "\n",
    "    def get_bytecodes_from_artifacts(self, contracts, commit_hash=None):\n",
    "        def determine_path(contract_name, commit_hash):\n",
    "            # Code to determine the correct path for the given contract name and commit hash\n",
    "            if commit_hash:\n",
    "                path = os.path.join(\n",
    "                    \"temp_artifacts\",\n",
    "                    f\"{commit_hash}_artifacts\",\n",
    "                    f\"{contract_name}.sol\",\n",
    "                )\n",
    "            else:\n",
    "                path = os.path.join(\n",
    "                    self.dir_path_to_artifacts,\n",
    "                    f\"{contract_name}.sol\",\n",
    "                )\n",
    "            return path\n",
    "\n",
    "        def find_jsons(path):\n",
    "            # Use glob to find all json files in the directory\n",
    "            json_files = glob.glob(os.path.join(path, \"*.json\"))\n",
    "\n",
    "            # Check if more than one json file was found\n",
    "            if len(json_files) > 1:\n",
    "                print(f\"Warning: More than one JSON file found in {path}.\")\n",
    "\n",
    "            # If there's at least one json file, read it and print the name\n",
    "            if json_files:\n",
    "                with open(json_files[0]) as f:\n",
    "                    data = json.load(f)\n",
    "                print(f\"Read data from {json_files[0]}\")\n",
    "\n",
    "                bytecode = data[\"deployedBytecode\"][\"object\"]\n",
    "                if bytecode == \"\":\n",
    "                    print(f\"Bytecode for contract '{contract_name}' is empty.\")\n",
    "\n",
    "            # Return the deployed bytecode and the file name\n",
    "            return bytecode\n",
    "\n",
    "            # Get the timestamp of the commit\n",
    "\n",
    "        timestamp, _ = self.get_commit_timestamp(self.current_repo_path, commit_hash)\n",
    "        bytecodes = []\n",
    "\n",
    "        for contract_name in contracts.keys():\n",
    "            path = determine_path(contract_name, commit_hash)\n",
    "            bytecode = find_jsons(path)\n",
    "\n",
    "            # Add the information to the bytecodes list\n",
    "            bytecodes.append(\n",
    "                {\n",
    "                    \"contract_name\": contract_name,\n",
    "                    \"commit_hash\": commit_hash,\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"bytecode\": bytecode,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df_bytecodes = pd.DataFrame(bytecodes)\n",
    "\n",
    "        return df_bytecodes\n",
    "\n",
    "    def get_onchain_bytecodes(self, contracts):\n",
    "        \"\"\"\n",
    "        Retrieves on-chain runtime bytecodes for given contracts\n",
    "\n",
    "        Args:\n",
    "            contracts (dict): dictionary of contract names and addresses.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with contract names and their on-chain runtime bytecodes\n",
    "        \"\"\"\n",
    "        onchain_bytecodes = {}\n",
    "\n",
    "        for contract_name, contract_address in contracts.items():\n",
    "            try:\n",
    "                # TODO use multicall\n",
    "                onchain_bytecode = self.w3.eth.get_code(contract_address).hex()\n",
    "                onchain_bytecodes[contract_name] = onchain_bytecode\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Failed to get on-chain bytecode for contract '{contract_name}' at address '{contract_address}'. Error: {str(e)}\"\n",
    "                )\n",
    "                # raise e\n",
    "\n",
    "        # Convert dictionary to DataFrame\n",
    "        df_onchain_bytecodes = pd.DataFrame(\n",
    "            list(onchain_bytecodes.items()), columns=[\"contract_name\", \"bytecode\"]\n",
    "        )\n",
    "\n",
    "        return df_onchain_bytecodes\n",
    "\n",
    "    def get_metadata_hash_bytecode(self, bytecode):\n",
    "        # todo fix prefix\n",
    "        prefix = \"a264\"\n",
    "        start_index = bytecode.find(prefix)\n",
    "\n",
    "        while start_index != -1:\n",
    "            try:\n",
    "                metadata_hex = bytecode[start_index:]\n",
    "                metadata_hash_bytecode = binascii.unhexlify(metadata_hex)\n",
    "\n",
    "                self.metadata[bytecode] = metadata_hash_bytecode\n",
    "\n",
    "                return metadata_hash_bytecode, start_index\n",
    "            except (ValueError, EOFError):\n",
    "                start_index = bytecode.find(prefix, start_index + 1)\n",
    "\n",
    "        return None, -1\n",
    "\n",
    "    def get_ipfs_hash(self, metadata_hash_bytecode):\n",
    "        cbor_decoded_metadata_hash_bytecode: dict = cbor2.loads(metadata_hash_bytecode)\n",
    "        hex_ipfs_hash = cbor_decoded_metadata_hash_bytecode[\"ipfs\"]\n",
    "\n",
    "        ipfs_hash = base58.b58encode(hex_ipfs_hash).decode(\"utf-8\")\n",
    "\n",
    "        return ipfs_hash\n",
    "\n",
    "    def get_metadata_from_ipfs(self, ipfs_hash):\n",
    "        # Use an IPFS gateway to fetch the data\n",
    "        response = requests.get(f\"https://ipfs.io/ipfs/{ipfs_hash}\")\n",
    "\n",
    "        # The actual metadata is usually a JSON file\n",
    "        metadata = response.json()\n",
    "\n",
    "        pprint.pprint(metadata)\n",
    "\n",
    "    def remove_metadata_hash_bytecode(self, bytecode):\n",
    "        md, idx = self.get_metadata_hash_bytecode(bytecode)\n",
    "        if idx != -1:\n",
    "            return bytecode[:idx]\n",
    "        return bytecode\n",
    "\n",
    "    def are_bytecodes_matching(self, bytecode1, bytecode2):\n",
    "        return bytecode1 == bytecode2\n",
    "\n",
    "    def compare_bytecodes(self, df_bytecodes1, df_bytecodes2):\n",
    "        \"\"\"\n",
    "        Compares bytecodes for each contract in two given dataframes\n",
    "\n",
    "        Args:\n",
    "            df_bytecodes1 (DataFrame): dataframe with contract names, commit hashes, timestamps, and their bytecodes\n",
    "            df_bytecodes2 (DataFrame): dataframe with contract names, commit hashes, timestamps, and their bytecodes\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with contract names, commit hashes, timestamps, and boolean values indicating if bytecodes are different\n",
    "        \"\"\"\n",
    "        compare_list = []\n",
    "\n",
    "        for index, row in df_bytecodes1.iterrows():\n",
    "            contract_name = row[\"contract_name\"]\n",
    "            bytecode1 = row[\"bytecode\"]\n",
    "\n",
    "            # Get the corresponding row from df_bytecodes2\n",
    "            df_row_bytecodes2 = df_bytecodes2[\n",
    "                df_bytecodes2[\"contract_name\"] == contract_name\n",
    "            ]\n",
    "\n",
    "            if df_row_bytecodes2.empty:\n",
    "                bytecode2 = \"\"\n",
    "            else:\n",
    "                bytecode2 = df_row_bytecodes2[\"bytecode\"].values[0]\n",
    "\n",
    "            compare_list.append(\n",
    "                {\n",
    "                    \"contract_name\": contract_name,\n",
    "                    \"commit_hash\": row[\"commit_hash\"],\n",
    "                    \"timestamp\": row[\"timestamp\"],\n",
    "                    \"are_bytecodes_different_w\": self.are_bytecodes_matching(\n",
    "                        bytecode1, bytecode2\n",
    "                    ),\n",
    "                    \"are_bytecodes_different_wo\": self.are_bytecodes_matching(\n",
    "                        self.remove_metadata_hash_bytecode(bytecode1),\n",
    "                        self.remove_metadata_hash_bytecode(bytecode2),\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(compare_list)\n",
    "\n",
    "    def compare_contract_bytecodes(self):\n",
    "        onchain_bytecodes = self.get_onchain_bytecodes(self.contracts)\n",
    "\n",
    "        # If bytecode_verification_report.csv exists, read it into a DataFrame\n",
    "        final_df = (\n",
    "            pd.read_csv(\"bytecode_verification_report.csv\")\n",
    "            if os.path.isfile(\"bytecode_verification_report.csv\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        for commit in self.build_previous_commits:\n",
    "            df_comparisons_config = self.compare_bytecodes(\n",
    "                self.get_bytecodes_from_artifacts(self.contracts, commit),\n",
    "                onchain_bytecodes,\n",
    "            )\n",
    "\n",
    "            # If the results for this commit already exist in the final_df, output a warning\n",
    "            if final_df is not None and commit in final_df.columns:\n",
    "                warnings.warn(f\"Comparison results for commit {commit} already exist.\")\n",
    "                continue\n",
    "\n",
    "            # Merge the new comparison results into final_df\n",
    "            if final_df is None:\n",
    "                final_df = df_comparisons_config\n",
    "            else:\n",
    "                final_df = final_df.merge(\n",
    "                    df_comparisons_config, on=\"contract_name\", how=\"outer\"\n",
    "                )\n",
    "\n",
    "        df_comparisons_current = self.compare_bytecodes(\n",
    "            self.get_bytecodes_from_artifacts(self.contracts),\n",
    "            onchain_bytecodes,\n",
    "        )\n",
    "\n",
    "        # Merge the current comparison results into final_df\n",
    "        if final_df is None:\n",
    "            final_df = df_comparisons_current\n",
    "        else:\n",
    "            final_df = final_df.merge(\n",
    "                df_comparisons_current, on=\"contract_name\", how=\"outer\"\n",
    "            )\n",
    "\n",
    "        if final_df is not None:\n",
    "            final_df.to_csv(\"bytecode_verification_report.csv\", index=False)\n",
    "        else:\n",
    "            print(\"No data to write to 'bytecode_verification_report.csv'.\")\n",
    "\n",
    "    def print_contracts_info(self, merged_dict):\n",
    "        base_url = \"https://etherscan.io/address/\"\n",
    "        active_facet_addresses = self.verify_contracts_are_active()\n",
    "\n",
    "        chain_id = self.w3.eth.chain_id\n",
    "        block_number = self.w3.eth.block_number\n",
    "        timestamp = self.w3.eth.get_block(block_number).timestamp\n",
    "        # Convert timestamp to datetime\n",
    "        dt = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "\n",
    "        # Convert datetime to ISO 8601 format\n",
    "        iso8601_time = dt.isoformat()\n",
    "\n",
    "        with open(\"bytecode_verification_report.md\", \"w\") as file:\n",
    "            file.write(\"# Bytecode Verification Report\\n\\n\")\n",
    "\n",
    "            if self.is_run_locally:\n",
    "                file.write(\"This report is generated locally.\\n\\n\")\n",
    "            else:\n",
    "                file.write(\"This report is generated from the CI pipeline.\\n\\n\")\n",
    "\n",
    "            file.write(\n",
    "                f\"## Network {chain_id}, block number {block_number} ({iso8601_time})\\n\\n\"\n",
    "            )\n",
    "\n",
    "            file.write(f\"Proxy address: `{self.diamond_address}`\\n\\n\")\n",
    "\n",
    "            file.write(\n",
    "                f\"Number of active facets: `{len(self.current_facet_addresses)}`\\n\\n\"\n",
    "            )\n",
    "\n",
    "            file.write(\n",
    "                \"| Contract Name | Address | Active | Current Commit Hash | Comparison with Metadata (Current Commit) | Comparison without Metadata (Current Commit) | Commit Hash (Config) | Comparison with Metadata (Config Commit) | Comparison without Metadata (Config Commit) |\\n\"\n",
    "            )\n",
    "            file.write(\n",
    "                \"|---------------|---------|--------|---------------------|---------------------------------------|-------------------------------------------|---------------------|-----------------------------------------|---------------------------------------------|\\n\"\n",
    "            )\n",
    "\n",
    "            for contract_name, info in merged_dict.items():\n",
    "                comparison_with_metadata_current = (\n",
    "                    \"✅\" if info[\"comparison_with_metadata_current\"] else \"❌\"\n",
    "                )\n",
    "                comparison_without_metadata_current = (\n",
    "                    \"✅\" if info[\"comparison_without_metadata_current\"] else \"❌\"\n",
    "                )\n",
    "                comparison_with_metadata_config = (\n",
    "                    \"✅\" if info[\"comparison_with_metadata_config\"] else \"❌\"\n",
    "                )\n",
    "                comparison_without_metadata_config = (\n",
    "                    \"✅\" if info[\"comparison_without_metadata_config\"] else \"❌\"\n",
    "                )\n",
    "                clickable_address = base_url + info[\"address\"] + \"#code\"\n",
    "                active = \"✅\" if active_facet_addresses[contract_name] else \"❌\"\n",
    "                commit_hash_config = info[\"commit_hash_config\"]\n",
    "\n",
    "                file.write(\n",
    "                    f\"| {contract_name} | [{info['address']}]({clickable_address}) | {active} | {self.current_commit_sha} | {comparison_with_metadata_current} | {comparison_without_metadata_current} | {commit_hash_config} | {comparison_with_metadata_config} | {comparison_without_metadata_config} |\\n\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_REPOSITORY = \"nayms/contracts-v3\"\n",
    "\n",
    "FORGE_PATH = \"/Users/kevinpark/.foundry/bin/forge\"\n",
    "REPO_PATH = os.path.join(\"/\", \"Users\", \"kevinpark\", \"dev\", \"nayms\", \"v3-extoken\")\n",
    "\n",
    "build_previous_commits = [\"9de0e394\"]\n",
    "\n",
    "web3_instance = Web3(\n",
    "    Web3.HTTPProvider(os.getenv(\"ETH_MAINNET_RPC_URL\"))\n",
    ")  # update with your provider details\n",
    "contract_abi = diamondLoupeABI\n",
    "dir_path_to_broadcasts = os.path.join(\n",
    "    REPO_PATH, \"broadcast\"\n",
    ")  # update with your broadcasts directory path\n",
    "dir_path_to_artifacts = os.path.join(\n",
    "    REPO_PATH, \"forge-artifacts\"\n",
    ")  # update with your artifacts directory path\n",
    "additional_contracts = {\n",
    "    \"NaymsOwnershipFacet\": \"0x073C1a072845D1d87f42309af9911bd3c07fC599\",\n",
    "    \"DiamondLoupeFacet\": \"0x0318ff107aFA55E3dc658cEA06748d0c35fbEC73\",\n",
    "}\n",
    "compare_bytecode = CompareBytecode(\n",
    "    build_previous_commits=build_previous_commits,\n",
    "    dir_path_to_broadcasts=dir_path_to_broadcasts,\n",
    "    dir_path_to_artifacts=dir_path_to_artifacts,\n",
    "    w3=web3_instance,\n",
    "    contract_abi=contract_abi,\n",
    "    additional_contracts=additional_contracts,\n",
    "    forge_path=FORGE_PATH,\n",
    "    repo_path=REPO_PATH,\n",
    ")\n",
    "\n",
    "compare_bytecode.compare_contract_bytecodes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contract address list\n",
    "Proxy address - optional if contract address list is provided\n",
    "\n",
    "\n",
    "Check if there has been new upgrades onchain since the last time this report has been run.\n",
    "\n",
    "Autotag:\n",
    "Proxy address has changed\n",
    "Upgrade - Implementation contracts have been added, replaced, removed\n",
    "Metadata change \n",
    "\n",
    "\n",
    "If this is the first time that this report will be generated, \n",
    "the user can seed commits to have the script check specific contracts from these commits.\n",
    "The script should always check the previous commits that match onchain bytecode to validate that this is still relevant for this report.\n",
    "\n",
    "If mismatching, then the script will check the report history to see the last commit in the report history matches, and rerun verification for these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
